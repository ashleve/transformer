{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06144b3a-2c0e-435c-95d3-2cddd9b05376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55aa0f90-6dfe-4c1b-b1fc-bb9e6fb42480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0c5ce8-130d-40d9-87e4-b649415da62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e96912-18d8-42bb-bb9d-6327be717d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "af59d687-87a8-418d-b9d7-2d69e1c1abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "\n",
    "def generate_vocabulary(data_dir=\"data/\"):\n",
    "    raw_text_iter = WikiText2(root=os.path.join(data_dir, \"wikitext\"), split=\"train\")\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    counter = Counter()\n",
    "    for x in raw_text_iter:\n",
    "        counter.update(tokenizer(x))\n",
    "    return Vocab(counter)\n",
    "\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    \"\"\"Wikipedia Language Modelling.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, split, vocab, seq_len=50):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "        data_iter = WikiText2(root=os.path.join(data_dir, \"wikitext\"), split=split)\n",
    "        self.train_data = self.data_process(data_iter)\n",
    "\n",
    "    def data_process(self, raw_text_iter):\n",
    "        data = [\n",
    "            torch.tensor([self.vocab[token] for token in self.tokenizer(item)], dtype=torch.long)\n",
    "            for item in raw_text_iter\n",
    "        ]\n",
    "        data = [x for x in data if x.numel() > 0]\n",
    "        return torch.cat((data))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.train_data[index * self.seq_len : (index + 1) * self.seq_len]\n",
    "        target = self.train_data[(index + 1) * self.seq_len : (index + 2) * self.seq_len]\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data) // self.seq_len - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "845b235d-8ab1-42b2-8318-295a4b2fe6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class WikiTextDataModule(LightningDataModule):\n",
    "    \"\"\"Wikipedia Language Modelling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"data/\",\n",
    "        batch_size: int = 64,\n",
    "        seq_len: int = 30,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "        drop_last: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Download data if needed.\"\"\"\n",
    "        # WikiTextDataset(self.data_dir, )\n",
    "        # WikiTextDataset(self.data_dir, )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Load data. Set variables: self.data_train, self.data_val, self.data_test.\"\"\"\n",
    "        self.vocab = generate_vocabulary(data_dir=self.data_dir)\n",
    "        self.data_train = WikiTextDataset(\n",
    "            self.data_dir, split=\"train\", vocab=self.vocab, seq_len=self.seq_len\n",
    "        )\n",
    "        self.data_val = WikiTextDataset(\n",
    "            self.data_dir, split=\"valid\", vocab=self.vocab, seq_len=self.seq_len\n",
    "        )\n",
    "        self.data_test = WikiTextDataset(\n",
    "            self.data_dir, split=\"test\", vocab=self.vocab, seq_len=self.seq_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            drop_last=self.drop_last,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            drop_last=self.drop_last,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            drop_last=self.drop_last,\n",
    "            shuffle=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b8e14b98-2740-43dd-86a7-33b87ba0e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = WikiTextDataModule(seq_len=100, batch_size=1)\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "42372131-9d4f-4710-bcd1-2fca4127b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "state that the habit of double spacing is too deeply <unk> to change . others claim that additional space between sentences improves the aesthetics or readability of text . proponents of double sentence spacing also state that some publishers may still require double @-@ spaced manuscript submissions from authors . a key example noted is the screenwriting industry ' s monospaced standard for screenplay manuscripts , courier , 12 @-@ point font , although some works on screenwriting indicate that courier is merely preferred â€“ proportional fonts may be used . some reliable sources state simply that writers should follow\n",
      "\n",
      "their particular style guide , but proponents of double spacing caution that publishers ' guidance takes precedence , including those that ask for double sentence spaced manuscripts . one of the most popular arguments against wider sentence spacing is that it was created for monospaced fonts of the typewriter , and is no longer needed with modern proportional fonts . however , proportional fonts existed together with wide sentence spacing for centuries before the typewriter , and remained for decades after its invention . when the typewriter was first introduced , typists were most commonly taught to use three spaces\n"
     ]
    }
   ],
   "source": [
    "for x, y in datamodule.train_dataloader():\n",
    "    print(x.shape)\n",
    "    sentence1 = [datamodule.vocab.itos[int(number)] for number in torch.flatten(x)]\n",
    "    print(' '.join(sentence1))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    sentence2 = [datamodule.vocab.itos[int(number)] for number in torch.flatten(y)]\n",
    "    print(' '.join(sentence2))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb958cca-82ef-4d2b-9430-99f6b485b2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ae7ca80f-3b89-4431-adeb-783d1e59cd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130768\n",
      "divorced\n"
     ]
    }
   ],
   "source": [
    "vocab = datamodule.vocab.freqs\n",
    "print(datamodule.vocab.freqs[\"the\"])\n",
    "\n",
    "sentence = \n",
    "\n",
    "print(list(vocab.keys())[list(vocab.values()).index(17)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1ba50963-3cbd-4538-9a49-ca9a1a93d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'def': 2, 'gh': 2, 'abc': 1})\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/\"\n",
    "raw_text_iter = [\"abc\", \"def\", \"gh\", \"gh\", \"def\"]\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "counter = Counter()\n",
    "for x in raw_text_iter:\n",
    "    counter.update(tokenizer(x))\n",
    "print(counter)\n",
    "vocab = Vocab(counter)\n",
    "print(vocab[\"def\"])\n",
    "print(vocab[\"gh\"])\n",
    "print(vocab[\"abc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a0816bcb-d2b2-475d-bca6-99fe79b0a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 30])\n",
      "tensor([   88,     3,    55,  2541,     3,   129,   149,    12,  1497,  7087,\n",
      "            3,  1487,     3, 10371,     6, 23132,     3, 17746,     4,    67,\n",
      "         1609,     2,    79,     3,  1280,    11,  3016,     9,   289,  2204])\n",
      "['city', ',', 'new', 'jersey', ',', 'united', 'states', \"'\", 'entertainment', 'resort', ',', 'hotel', ',', 'casino', 'and', 'spa', ',', 'revel', '.', 'while', 'singing', 'the', 'song', ',', 'beyoncÃ©', 'was', 'wearing', 'a', 'black', 'dress']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "40c3bdbd-ebfa-4ab1-8453-d55653467c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size))\n",
    "    mask = torch.flip(mask, dims=(-1,))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ae5c08d0-0f6e-472b-9f28-e5c479af0ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_square_mask(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8df3ae90-d8fd-457c-9270-25af1ba7bebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3388dfd3-a5af-4f7e-b983-c4c49595b106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30751901-ef42-470b-83fe-c180787e25b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1f8b85-9f19-48c9-b1eb-31c7d1bf1d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric\n",
    "datasets_list = list_datasets()\n",
    "len(datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362ef7f2-aa6e-418b-8cf2-ad7fe075a455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/ash/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a18a806d-2158-473c-9bb0-5400a04adc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4358\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3000281a-b6d4-4db5-bc23-1a5206bb1ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ''}\n",
      "{'text': ' = Robert Boulter = \\n'}\n",
      "{'text': ''}\n",
      "{'text': ' Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \\n'}\n",
      "{'text': ' In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . Boulter starred in the 2011 film Mercenaries directed by Paris Leonti . \\n'}\n",
      "{'text': ''}\n"
     ]
    }
   ],
   "source": [
    "haha = iter(dataset[\"test\"])\n",
    "print(next(haha))\n",
    "print(next(haha))\n",
    "print(next(haha))\n",
    "print(next(haha))\n",
    "print(next(haha))\n",
    "print(next(haha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "849287d1-4b62-4540-b089-8d722e5a36ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': (4358, 1), 'train': (36718, 1), 'validation': (3760, 1)}\n",
      "{'test': 1, 'train': 1, 'validation': 1}\n",
      "{'test': 4358, 'train': 36718, 'validation': 3760}\n",
      "{'test': ['text'], 'train': ['text'], 'validation': ['text']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(dataset.num_columns)\n",
    "print(dataset.num_rows)\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4f59560-60cd-4617-a24c-5376e7aed13d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-267b2b7c1495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'description'"
     ]
    }
   ],
   "source": [
    "dataset.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb28130b-a99e-4aa3-a613-4c1bd4763d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 5, 2, 5, 2, 5]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset[\"test\"][\"text\"][:300]\n",
    "\n",
    "abc = [3, 2 ,5 ,2 ,5, 2, 5]\n",
    "abc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ecc3d-d24a-494f-8610-50c3a2cdb1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
