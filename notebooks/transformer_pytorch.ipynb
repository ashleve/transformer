{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06144b3a-2c0e-435c-95d3-2cddd9b05376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55aa0f90-6dfe-4c1b-b1fc-bb9e6fb42480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0c5ce8-130d-40d9-87e4-b649415da62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e96912-18d8-42bb-bb9d-6327be717d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028cc5e-9cc0-416e-905a-c2f961fcf58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb1edd-b2f4-4479-8f45-9e7c790e61ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b05d2cf-4a06-427e-a4ea-05d1ceff0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter, val_iter, test_iter = WikiText2()\n",
    "\n",
    "# i = 0\n",
    "# for x in test_iter:\n",
    "#     print(x)\n",
    "#     i += 1\n",
    "#     if i > 100:\n",
    "#         break\n",
    "\n",
    "# train_data = data_process(train_iter)\n",
    "# val_data = data_process(val_iter)\n",
    "# test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b953c84-ec8c-4a46-b0f3-35e8a61f5bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d42528f-2941-47c4-a336-2accc23e983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "def generate_vocabulary(raw_text_iter):\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    counter = Counter()\n",
    "    for x in raw_text_iter:\n",
    "        counter.update(tokenizer(x))\n",
    "    return Vocab(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17132ace-8743-4b1a-9ab3-12616fc45575",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(root=\"../data/wikitext\", split=\"train\")\n",
    "vocab = generate_vocabulary(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af59d687-87a8-418d-b9d7-2d69e1c1abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class WikitextDataset(Dataset):\n",
    "    \"\"\"Wikipedia Language Modelling.\"\"\"\n",
    "        \n",
    "    def __init__(self, root, split, vocab, tokenizer, seq_len=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        \n",
    "        data_iter = WikiText2(root=root, split=split)\n",
    "        self.train_data = self.data_process(data_iter)       \n",
    "        \n",
    "    def data_process(self, raw_text_iter):\n",
    "        data = [torch.tensor([self.vocab[token] for token in self.tokenizer(item)], dtype=torch.long) for item in raw_text_iter]\n",
    "        data = [x for x in data if x.numel() > 0]\n",
    "        return torch.cat((data)) \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.train_data[index*self.seq_len:(index+1)*self.seq_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data) // self.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "845b235d-8ab1-42b2-8318-295a4b2fe6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837\n",
      "214417\n"
     ]
    }
   ],
   "source": [
    "trainset = WikitextDataset(root=\"../data/wikitext\", split=\"train\", vocab=vocab, tokenizer=tokenizer, seq_len=256)\n",
    "trainset = WikitextDataset(root=\"../data/wikitext\", split=\"valid\", vocab=vocab, tokenizer=tokenizer, seq_len=256)\n",
    "print(len(trainset))\n",
    "print(len(trainset.train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ba2965c-2e8d-4dd8-ba09-e5fd1e6cb5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "loader = DataLoader(trainset, batch_size=8)\n",
    "# print(next(iter(loader)))\n",
    "print(next(iter(loader)).size())\n",
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c3bdbd-ebfa-4ab1-8453-d55653467c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5c08d0-0f6e-472b-9f28-e5c479af0ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df3ae90-d8fd-457c-9270-25af1ba7bebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3388dfd3-a5af-4f7e-b983-c4c49595b106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30751901-ef42-470b-83fe-c180787e25b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1f8b85-9f19-48c9-b1eb-31c7d1bf1d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric\n",
    "datasets_list = list_datasets()\n",
    "len(datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362ef7f2-aa6e-418b-8cf2-ad7fe075a455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/ash/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a18a806d-2158-473c-9bb0-5400a04adc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 4358\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3000281a-b6d4-4db5-bc23-1a5206bb1ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ''}\n",
      "{'text': ' = Robert Boulter = \\n'}\n",
      "{'text': ''}\n",
      "{'text': ' Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \\n'}\n",
      "{'text': ' In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . Boulter starred in the 2011 film Mercenaries directed by Paris Leonti . \\n'}\n",
      "{'text': ''}\n"
     ]
    }
   ],
   "source": [
    "haha = iter(dataset[\"test\"])\n",
    "print(next(haha))\n",
    "print(next(haha))\n",
    "print(next(haha))\n",
    "print(next(haha))\n",
    "print(next(haha))\n",
    "print(next(haha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "849287d1-4b62-4540-b089-8d722e5a36ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': (4358, 1), 'train': (36718, 1), 'validation': (3760, 1)}\n",
      "{'test': 1, 'train': 1, 'validation': 1}\n",
      "{'test': 4358, 'train': 36718, 'validation': 3760}\n",
      "{'test': ['text'], 'train': ['text'], 'validation': ['text']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(dataset.num_columns)\n",
    "print(dataset.num_rows)\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4f59560-60cd-4617-a24c-5376e7aed13d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-267b2b7c1495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'description'"
     ]
    }
   ],
   "source": [
    "dataset.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb28130b-a99e-4aa3-a613-4c1bd4763d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 5, 2, 5, 2, 5]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset[\"test\"][\"text\"][:300]\n",
    "\n",
    "abc = [3, 2 ,5 ,2 ,5, 2, 5]\n",
    "abc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ecc3d-d24a-494f-8610-50c3a2cdb1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
